{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chousallen/MachineLearning2025/blob/hw3-test/hw03/ML_HW3_understanding_LLM_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkELbiG6ogiG"
      },
      "source": [
        "# Understanding LLM / Transformers (You cannot run the code without saving a copy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvlATdaN45U8"
      },
      "source": [
        "## Check the status of your GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "To39dynktNC2",
        "collapsed": true,
        "outputId": "7769cbdb-1303-40c5-9d00-1922725b02b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Apr  2 13:12:37 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   77C    P0             30W /   70W |    5560MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FISzjhfg4vo6"
      },
      "source": [
        "## Installing **transformers** for further usage (please do not alter the version for stable usage of model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "e0ZtRUtLoWjq",
        "collapsed": true,
        "outputId": "2364cfe2-e2de-4194-b449-6af8577ec4fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.47.0 in /usr/local/lib/python3.11/dist-packages (4.47.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.0) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.0) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.47.0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.47.0) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.47.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.47.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.47.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.47.0) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.47.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLvm_l4g4O9U"
      },
      "source": [
        "## Huggingface login\n",
        "\n",
        "### You need the huggingface token (hf_token) to login to huggingface and install the gemma model. Therefore make sure you create your huggingface token. (Described in the Google slides)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9ewrDfyEr0KS",
        "outputId": "49b4366a-2385-4026-92a3-9919a46dab94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "Invalid user token.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/whoami-v2",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mwhoami\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m   1663\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1664\u001b[0;31m             \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1665\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0;31m# as well (request id and/or server error message)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/whoami-v2 (Request ID: Root=1-67ed37ca-76ad16de6e3383ac582cc6c7;b258923c-82af-46ce-a3f6-c74e220d0c36)\n\nInvalid credentials in Authorization header",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-ae90f891e49f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"your_hf_token\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#######################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"\\n\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcustom_message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             args_msg = [\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/_login.py\u001b[0m in \u001b[0;36mlogin\u001b[0;34m(token, add_to_git_credential, new_session, write_permission)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;34m\"you want to set the git credential as well.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             )\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0m_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_to_git_credential\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_to_git_credential\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mnotebook_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_session\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/_login.py\u001b[0m in \u001b[0;36m_login\u001b[0;34m(token, add_to_git_credential)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You must use your personal account token, not an organization token.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m     \u001b[0mtoken_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhoami\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m     \u001b[0mpermission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auth\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accessToken\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Token is valid (permission: {permission}).\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mwhoami\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m   1675\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0meffective_token\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_get_token_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1676\u001b[0m                 \u001b[0merror_message\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" The token stored is invalid. Please run `huggingface-cli login` to update it.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1677\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1678\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: Invalid user token."
          ]
        }
      ],
      "source": [
        "######################## TODO (Pre-requisites) ########################\n",
        "# replace `your_hf_token` with your huggingface token\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(\"your_hf_token\")\n",
        "#######################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the model"
      ],
      "metadata": {
        "id": "HY1e8Urn-qy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gemma Model: https://huggingface.co/google/gemma-2-2b-it\n",
        "### Please accept the lincense to download the gemma model (As described on Google Slides)\n"
      ],
      "metadata": {
        "id": "N_nHl3zFCS5T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrrBTIl-ryeo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_id = \"google/gemma-2-2b-it\"\n",
        "dtype = torch.float16\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=dtype,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1: Chat template Comparison"
      ],
      "metadata": {
        "id": "P4cI-w4lEqvz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Model: https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2\n"
      ],
      "metadata": {
        "id": "iBlbhVN4BHVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "SCORING_MODEL = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "SCORING_TOKENIZER = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "\n",
        "def calculate_coherence(question, answer, scoring_model=SCORING_MODEL, tokenizer=SCORING_TOKENIZER):\n",
        "  features = tokenizer([question], [answer], padding=True, truncation=True, return_tensors=\"pt\")\n",
        "  scoring_model.eval()\n",
        "  with torch.no_grad():\n",
        "      scores = scoring_model(**features).logits.squeeze().item()\n",
        "  return scores"
      ],
      "metadata": {
        "id": "-iSpRNhLBcMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JFRJMWdMrHU"
      },
      "source": [
        "### Observe whether the chat template affects the model's output results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl-FK-lycGs5"
      },
      "outputs": [],
      "source": [
        "def generate_text_from_prompt(prompt, tokenizer, model):\n",
        "  \"\"\"\n",
        "  generate the output from the prompt.\n",
        "  param:\n",
        "    prompt (str): the prompt inputted to the model\n",
        "    tokenizer   : the tokenizer that is used to encode / decode the input / output\n",
        "    model       : the model that is used to generate the output\n",
        "\n",
        "  return:\n",
        "    the response of the model\n",
        "  \"\"\"\n",
        "  print(\"========== Prompt inputted to the model ==========\\n\", prompt)\n",
        "\n",
        "  # Tokenize the prompt\n",
        "  input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "  ######################## TODO (Q1.1 ~ 1.4) ########################\n",
        "  ### You can refer to https://huggingface.co/google/gemma-2-2b-it for basic usage\n",
        "  ### Make sure to use 'do_sample=False' to get a deterministic response\n",
        "  ### Otherwise the coherence score may be different from the sample answer\n",
        "\n",
        "  # Generate response\n",
        "  output_ids =\n",
        "  ###################################################################\n",
        "  if output_ids is not None and len(output_ids) > 0:\n",
        "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "  else:\n",
        "    return \"Empty Response\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_Gw3DqEbe6B"
      },
      "outputs": [],
      "source": [
        "# With chat template\n",
        "question = \"Please tell me about the key differences between supervised learning and unsupervised learning. Answer in 200 words.\"\n",
        "chat = [\n",
        "    {\"role\": \"user\", \"content\": question},\n",
        "]\n",
        "prompt_with_template = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "response_with_template = generate_text_from_prompt(prompt_with_template, tokenizer, model)\n",
        "\n",
        "# extract the real output from the model\n",
        "response_with_template = response_with_template.split('model\\n')[-1].strip('\\n').strip()\n",
        "\n",
        "print(\"========== Output ==========\\n\", response_with_template)\n",
        "score = calculate_coherence(question, response_with_template)\n",
        "print(f\"========== Coherence Score : {score:.4f}  ==========\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aaeWDSCbigu"
      },
      "outputs": [],
      "source": [
        "# Without chat template (directly using plain text)\n",
        "response_without_template = generate_text_from_prompt(question, tokenizer, model)\n",
        "\n",
        "# extract the real output from the model\n",
        "response_without_template = response_without_template.split(question.split(' ')[-1])[-1].strip('\\n').strip()\n",
        "print(\"========== Output ==========\\n\", response_without_template)\n",
        "score = calculate_coherence(question, response_without_template)\n",
        "print(f\"========== Coherence Score : {score:.4f}  ==========\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YT8amEJSP-E"
      },
      "source": [
        "## Q2: Multi-turn conversations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAkqoPxsSrbz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "chat_history = []\n",
        "round = 0\n",
        "print(\"Chatbot: Hello! How can I assist you today? (Type 'exit' to quit)\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        print(\"Chatbot: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    round += 1\n",
        "    chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "    chat_template_format_prompt = tokenizer.apply_chat_template(chat_history, tokenize=False, add_generation_prompt=True)\n",
        "    ######################## (Q2.1 ~ 2.3) ########################\n",
        "    # Observe the prompt with chat template format that was inputted to the model in the current round to answer Q2.1 ~ Q2.3.\n",
        "    print(f\"=== Prompt with chat template format inputted to the model on round {round} ===\\n{chat_template_format_prompt}\")\n",
        "    print(f\"===============================================\")\n",
        "    ###################################################################\n",
        "\n",
        "    inputs = tokenizer(chat_template_format_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    # Get logits instead of directly generating\n",
        "    with torch.no_grad():\n",
        "        outputs_p = model(**inputs)\n",
        "\n",
        "    logits = outputs_p.logits  # Logits of the model (raw scores before softmax)\n",
        "    last_token_logits = logits[:, -1, :]  # Take the logits of the last generated token\n",
        "\n",
        "    # Apply softmax to get probabilities\n",
        "    probs = torch.nn.functional.softmax(last_token_logits, dim=-1)\n",
        "\n",
        "    # Get top-k tokens (e.g., 10)\n",
        "    top_k = 10\n",
        "    top_probs, top_indices = torch.topk(probs, top_k)\n",
        "\n",
        "    # Convert to numpy for plotting\n",
        "    top_probs = top_probs.cpu().squeeze().numpy()\n",
        "    top_indices = top_indices.cpu().squeeze().numpy()\n",
        "    top_tokens = [tokenizer.decode([idx]) for idx in top_indices]\n",
        "\n",
        "    # Plot probability distribution\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.barplot(x=top_probs, y=top_tokens, palette=\"coolwarm\")\n",
        "    plt.xlabel(\"Probability\")\n",
        "    plt.ylabel(\"Token\")\n",
        "    plt.title(\"Top Token Probabilities for Next Word\")\n",
        "    plt.show()\n",
        "\n",
        "    # Generate response\n",
        "    outputs = model.generate(**inputs, max_new_tokens=200, pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
        "    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "    print(f\"Chatbot: {response}\")\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": response})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo7JKIgp0Txd"
      },
      "source": [
        "## Q3: Tokenization of Sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHrVdb_20Txd"
      },
      "outputs": [],
      "source": [
        "sentence = \"I love taking a Machine Learning course by Professor Hung-yi Lee, What about you?\" #@param {type:\"string\"}\n",
        "\n",
        "######################## TODO (Q3.1 ~ 3.4) ########################\n",
        "### You can refer to https://huggingface.co/learn/nlp-course/en/chapter2/4?fw=pt for basic tokenizer usage\n",
        "### and https://huggingface.co/docs/transformers/en/main_classes/tokenizer for full tokenizer usage\n",
        "\n",
        "\n",
        "# Encode the sentence into token IDs without adding special tokens\n",
        "token_ids =\n",
        "\n",
        "# Convert the token IDs back to their corresponding tokens (words or subwords)\n",
        "tokens =\n",
        "###################################################################\n",
        "\n",
        "# Iterate through the tokens and their corresponding token IDs\n",
        "for t, t_id in zip(tokens, token_ids):\n",
        "    # Print the token and its index (ID)\n",
        "    print(f\"Token: {t}, token index: {t_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1ocRPtU0Txe"
      },
      "source": [
        "## Q4: Auto-regressive generation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import trange\n",
        "from transformers import HybridCache\n",
        "\n",
        "max_generation_tokens = 30\n",
        "\n",
        "######################## TODO (Q4.3 ~ 4.6) ########################\n",
        "# Modify the value of k and p accordingly\n",
        "\n",
        "top_k = 2  # Set K for top-k sampling\n",
        "top_p = 0.6  # Set P for nucleus sampling\n",
        "###################################################################\n",
        "\n",
        "# Input prompt\n",
        "prompt = f\"Generate a paraphrase of the sentence 'Professor Hung-yi Lee is one of the best teachers in the domain of machine learning'. Just response with one sentence.\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Initialize KV Cache\n",
        "kv_cache = HybridCache(config=model.config, max_batch_size=1, max_cache_len=max_generation_tokens, device=\"cuda\", dtype=torch.float16)\n",
        "\n",
        "next_token_id = input_ids.input_ids.to(\"cuda\")\n",
        "attention_mask = input_ids.attention_mask.to(\"cuda\")\n",
        "cache_position = torch.arange(attention_mask.shape[1], device=\"cuda\")\n",
        "\n",
        "generated_sentences_top_k = []\n",
        "generated_sentences_top_p = []\n",
        "\n",
        "\n",
        "\n",
        "# Define the generation parameters\n",
        "generation_params = {\n",
        "    \"do_sample\": True,  # Enable sampling\n",
        "    \"max_length\": max_generation_tokens + len(input_ids.input_ids[0]),  # Total length including prompt\n",
        "    \"pad_token_id\": tokenizer.pad_token_id,  # Ensure padding token is set\n",
        "    \"eos_token_id\": tokenizer.eos_token_id,  # Ensure EOS token is set\n",
        "    \"bos_token_id\": tokenizer.bos_token_id,  # Ensure BOS token is set\n",
        "    \"attention_mask\": input_ids.attention_mask.to(\"cuda\"),  # Move attention mask to GPU\n",
        "    \"use_cache\": True,  # Enable caching\n",
        "    \"return_dict_in_generate\": True,  # Return generation outputs\n",
        "    \"output_scores\": False,  # Disable outputting scores\n",
        "}\n",
        "\n",
        "\n",
        "for method in [\"top-k\", \"top-p\"]:\n",
        "    for _ in trange(20):\n",
        "      if method == \"top-k\":\n",
        "        # Generate text using the model with top_k\n",
        "        generated_output = model.generate(\n",
        "            input_ids=input_ids.input_ids.to(\"cuda\"),\n",
        "            top_k=top_k,\n",
        "            **generation_params\n",
        "        )\n",
        "      elif method == \"top-p\":\n",
        "        # Generate text using the model with top_p\n",
        "        ######################## TODO (Q4.3 ~ 4.6) ########################\n",
        "        # Generate output from the model based on the input_ids and specified generation parameters\n",
        "        # You can refer to this documentation: https://huggingface.co/docs/transformers/en/main_classes/text_generation\n",
        "        # Hint: You can check how we generate the text with top_k\n",
        "\n",
        "        generated_output =\n",
        "        ###################################################################\n",
        "      else:\n",
        "        raise NotImplementedError()\n",
        "      # Decode the generated tokens\n",
        "      generated_tokens = generated_output.sequences[0, len(input_ids.input_ids[0]):]\n",
        "      decoded_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "      # Combine the prompt with the generated text\n",
        "      sentence = decoded_text.replace(\" ,\", \",\").replace(\" 's\", \"'s\").replace(\" .\", \".\").strip()\n",
        "\n",
        "      # Append the generated sentence to the appropriate list\n",
        "      if method == \"top-k\":\n",
        "          generated_sentences_top_k.append(sentence)\n",
        "      else:\n",
        "          generated_sentences_top_p.append(sentence)\n",
        "\n",
        "# Print results\n",
        "print(\"===== Top-K Sampling Output =====\")\n",
        "print()\n",
        "for idx,sentence in enumerate(generated_sentences_top_k):\n",
        "    print(f\"{idx}. {sentence}\")\n",
        "print()\n",
        "print(\"===== Top-P Sampling Output =====\")\n",
        "print()\n",
        "for idx,sentence in enumerate(generated_sentences_top_p):\n",
        "    print(f\"{idx}. {sentence}\")\n",
        "print()"
      ],
      "metadata": {
        "id": "F3F8yl41t2he"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def compute_self_bleu(generated_sentences):\n",
        "    total_bleu_score = 0\n",
        "    num_sentences = len(generated_sentences)\n",
        "\n",
        "    for i, hypothesis in enumerate(generated_sentences):\n",
        "        references = [generated_sentences[j] for j in range(num_sentences) if j != i]\n",
        "        bleu_scores = [sentence_bleu([ref.split()], hypothesis.split()) for ref in references]\n",
        "        total_bleu_score += sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "    return total_bleu_score / num_sentences\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu_score = compute_self_bleu(generated_sentences_top_k)\n",
        "print(f\"self-BLEU Score for top_k (k={top_k}): {bleu_score:.4f}\")\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu_score = compute_self_bleu(generated_sentences_top_p)\n",
        "print(f\"self-BLEU Score for top_p (p={top_p}): {bleu_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "oMkYXsyLqR5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvI2LvarYe-M"
      },
      "source": [
        "## Q5: t-SNE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "######################## (Q5.2 ~ 5.3) ########################\n",
        "# Sentences with different meanings of words\n",
        "sentences = [\n",
        "    \"I ate a fresh apple.\",  # Apple (fruit)\n",
        "    \"Apple released the new iPhone.\",  # Apple (company)\n",
        "    \"I peeled an orange and ate it.\",  # Orange (fruit)\n",
        "    \"The Orange network has great coverage.\",  # Orange (telecom)\n",
        "    \"Microsoft announced a new update.\",  # Microsoft (company)\n",
        "    \"Banana is my favorite fruit.\",  # Banana (fruit)\n",
        "]\n",
        "\n",
        "# Tokenize and move to device\n",
        "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "inputs = inputs.to(device)\n",
        "\n",
        "# Get hidden states\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs, output_hidden_states=True)\n",
        "\n",
        "hidden_states = outputs.hidden_states[-1]  # Extract last layer embeddings\n",
        "\n",
        "# Compute sentence-level embeddings (mean pooling)\n",
        "sentence_embeddings = hidden_states.mean(dim=1).cpu().numpy()\n",
        "\n",
        "# Words to visualize\n",
        "word_labels = [\n",
        "    \"Apple (fruit)\", \"Apple (company)\",\n",
        "    \"Orange (fruit)\", \"Orange (telecom)\",\n",
        "    \"Microsoft (company)\", \"Banana (fruit)\"\n",
        "]\n",
        "\n",
        "# Reduce to 2D using t-SNE\n",
        "tsne = TSNE(n_components=2, perplexity=2, random_state=42)\n",
        "embeddings_2d = tsne.fit_transform(sentence_embeddings)\n",
        "\n",
        "# Plot the embeddings\n",
        "plt.figure(figsize=(8, 6))\n",
        "colors = [\"red\", \"blue\", \"orange\", \"purple\", \"green\", \"brown\"]\n",
        "for i, label in enumerate(word_labels):\n",
        "    plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1], color=colors[i], s=100)\n",
        "    plt.text(embeddings_2d[i, 0] + 0.1, embeddings_2d[i, 1] + 0.1, label, fontsize=12, color=colors[i])\n",
        "\n",
        "plt.xlabel(\"t-SNE Dim 1\")\n",
        "plt.ylabel(\"t-SNE Dim 2\")\n",
        "plt.title(\"t-SNE Visualization of Word Embeddings\")\n",
        "plt.show()\n",
        "##################################################"
      ],
      "metadata": {
        "id": "AH4t64HSv6r9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYDqomF2YWyX"
      },
      "source": [
        "## Q6: Observe the Attention Weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "R9nEUAHOYRU_"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from tqdm import trange\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Input prompt for text generation\n",
        "prompt = \"Google \"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\")  # Tokenize the input prompt\n",
        "next_token_id = input_ids.input_ids.to(\"cuda\")  # Move input token ids to GPU\n",
        "attention_mask = input_ids.attention_mask.to(\"cuda\")  # Move attention mask to GPU\n",
        "cache_position = torch.arange(attention_mask.shape[1], device=\"cuda\")  # Position for the KV cache\n",
        "\n",
        "# Set the number of tokens to generate and other parameters\n",
        "generation_tokens = 20  # Limit for visualization (number of tokens to generate)\n",
        "total_tokens = generation_tokens + next_token_id.size(1) - 1  # Total tokens to handle\n",
        "layer_idx = 10  # Specify the layer index for attention visualization\n",
        "head_idx = 7  # Specify the attention head index to visualize\n",
        "\n",
        "# KV cache setup for caching key/values across time steps\n",
        "from transformers.cache_utils import HybridCache\n",
        "kv_cache = HybridCache(config=model.config, max_batch_size=1, max_cache_len=total_tokens, device=\"cuda\", dtype=torch.float16)\n",
        "\n",
        "generated_tokens = []  # List to store generated tokens\n",
        "attentions = None  # Placeholder to store attention weights\n",
        "\n",
        "num_new_tokens = 0  # Counter for the number of new tokens generated\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Generate tokens and collect attention weights for visualization\n",
        "for num_new_tokens in range(generation_tokens):\n",
        "    with torch.no_grad():  # Disable gradients during inference for efficiency\n",
        "        # Pass the input through the model to get the next token prediction and attention weights\n",
        "        outputs = model(\n",
        "            next_token_id,\n",
        "            attention_mask=attention_mask,\n",
        "            cache_position=cache_position,\n",
        "            use_cache=True,  # Use the KV cache for efficiency\n",
        "            past_key_values=kv_cache,  # Provide the cached key-value pairs for fast inference\n",
        "            output_attentions=True  # Enable the extraction of attention weights\n",
        "        )\n",
        "\n",
        "    ######################## TODO (Q6.1 ~ 6.4) ########################\n",
        "    ### You can refer to https://huggingface.co/docs/transformers/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput.attentions to see the structure of model output attentions\n",
        "    # Get the logits for the last generated token from outputs\n",
        "    logits =\n",
        "    # Extract the attention scores from the model's outputs\n",
        "    attention_scores =\n",
        "    ###################################################################\n",
        "\n",
        "    # Extract attention weights for the specified layer and head\n",
        "    last_layer_attention = attention_scores[layer_idx][0][head_idx].detach().cpu().numpy()\n",
        "\n",
        "    # If it's the first generated token, initialize the attentions array\n",
        "    if num_new_tokens == 0:\n",
        "        attentions = last_layer_attention\n",
        "    else:\n",
        "        # Append the current attention weights to the existing array\n",
        "        attentions = np.append(attentions, last_layer_attention, axis=0)\n",
        "\n",
        "    # Choose the next token to generate based on the highest probability (logits)\n",
        "    next_token_id = logits.argmax(dim=-1)\n",
        "    generated_tokens.append(next_token_id.item())  # Add the token ID to the generated tokens list\n",
        "\n",
        "    # Update the attention mask and next token ID for the next iteration\n",
        "    attention_mask = torch.cat([attention_mask, torch.ones(1, 1, device=\"cuda\")], dim=-1)  # Add a new attention mask for the generated token\n",
        "    next_token_id = next_token_id.unsqueeze(0)  # Convert the token ID to the required shape\n",
        "\n",
        "    # Update the KV cache with the new past key-values\n",
        "    kv_cache = outputs.past_key_values\n",
        "    cache_position = cache_position[-1:] + 1  # Update the cache position for the next iteration\n",
        "\n",
        "# Decode the generated tokens into human-readable text\n",
        "generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "full_text = prompt + generated_text  # Combine the prompt with the generated text\n",
        "\n",
        "# Tokenize all the generated text (prompt + generated)\n",
        "tokens = tokenizer.tokenize(full_text)\n",
        "\n",
        "# Function to plot a heatmap of attention weights\n",
        "def plot_attention(attn_matrix, tokens, title=\"Attention Heatmap\"):\n",
        "    plt.figure(figsize=(10, 8))  # Set the figure size\n",
        "    sns.heatmap(attn_matrix, xticklabels=tokens, yticklabels=tokens, cmap=\"viridis\", annot=False)  # Plot the attention matrix as a heatmap\n",
        "    plt.xlabel(\"Key Tokens\")\n",
        "    plt.ylabel(\"Query Tokens\")\n",
        "    plt.title(title)\n",
        "    plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
        "    plt.yticks(rotation=0)  # Rotate y-axis labels\n",
        "    plt.show()\n",
        "\n",
        "# Plot the attention heatmap for the last generated token\n",
        "plot_attention(attentions, tokens, title=f\"Attention Weights for Generated Token of Layer {layer_idx}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz2z5Q4TYlyE"
      },
      "source": [
        "## Q7: Observe the Activation Scores"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code is referred from official Gemma tutorials: [Gemma Tutorial From Scratch](https://colab.research.google.com/drive/17dQFYUYnuKnP6OwQPH9v_GSYUW5aj-Rp#scrollTo=2-i7YRVLgKoT) and [SAELens](https://github.com/jbloomAus/SAELens/blob/main/tutorials/tutorial_2_0.ipynb)"
      ],
      "metadata": {
        "id": "Q2li0_FBwbit"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1a3DM4RXYxcE"
      },
      "outputs": [],
      "source": [
        "!pip install sae-lens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8iz9tluYuQy"
      },
      "outputs": [],
      "source": [
        "from sae_lens import SAE\n",
        "\n",
        "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
        "    release = \"gemma-scope-2b-pt-res-canonical\",\n",
        "    sae_id = \"layer_20/width_16k/canonical\",\n",
        ")\n",
        "\n",
        "print(sae, cfg_dict, sparsity)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YusIpmQYk-k"
      },
      "outputs": [],
      "source": [
        "from IPython.display import IFrame\n",
        "html_template = \"https://neuronpedia.org/{}/{}/{}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
        "\n",
        "def get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=\"20-gemmascope-res-16k\", feature_idx=0):\n",
        "    return html_template.format(sae_release, sae_id, feature_idx)\n",
        "\n",
        "########################## TODO (Q7.1) ############################\n",
        "html = get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=\"20-gemmascope-res-16k\", feature_idx=10004)\n",
        "IFrame(html, width=1200, height=600)\n",
        "###################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q7.2~7.3: Maximum activations comparison"
      ],
      "metadata": {
        "id": "51VLXHjtGRLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######################## (Q7.2 ~ 7.3) ########################\n",
        "\n",
        "def get_max_activation(model, tokenizer, sae, prompt, feature_idx=10004):\n",
        "    \"\"\"\n",
        "    Computes the maximum activation of a specific feature in a Sparse Autoencoder (SAE)\n",
        "    for a given prompt.\n",
        "\n",
        "    Args:\n",
        "        model: The Transformer model used for generating hidden states.\n",
        "        tokenizer: The tokenizer for encoding the prompt.\n",
        "        sae: The Sparse Autoencoder for encoding hidden states.\n",
        "        prompt (str): The input text prompt.\n",
        "        feature_idx (int, optional): The index of the feature in SAE. Defaults to 10004.\n",
        "\n",
        "    Returns:\n",
        "        float: The maximum activation value for the specified feature index.\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    sae.to(device)\n",
        "\n",
        "    # Tokenize the input prompt and get model outputs\n",
        "    tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model(tokens, output_hidden_states=True)\n",
        "\n",
        "    # Extract hidden states from the specified layer\n",
        "    hidden_states = outputs.hidden_states[sae.cfg.hook_layer]\n",
        "\n",
        "    # Encode hidden states using SAE\n",
        "    sae_in = hidden_states\n",
        "    feature_acts = sae.encode(sae_in).squeeze()  # Shape: (batch_size * seq_len, num_features)\n",
        "    feature_acts = feature_acts.reshape(-1, feature_acts.shape[-1])\n",
        "\n",
        "    # Compute max activation for the specified feature index\n",
        "    max_activation = -float(\"inf\")\n",
        "    batch_max_activation = feature_acts[:, feature_idx].max().item()\n",
        "    max_activation = max(max_activation, batch_max_activation)\n",
        "\n",
        "    # Plot activation distribution\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.hist(feature_acts[:, feature_idx].cpu().detach().numpy(), bins=50, alpha=0.75, color='blue', edgecolor='black')\n",
        "    plt.xlabel(f\"Activation values (Feature {feature_idx})\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.title(f\"Activation Distribution for Feature {feature_idx} - Prompt: '{prompt}'\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return max_activation\n",
        "\n",
        "feature_idx = 10004\n",
        "# Define the prompts\n",
        "prompt_a = \"Time travel offers me the opportunity to correct past errors, but it comes with its own set of risks.\"\n",
        "prompt_b = \"I accept that my decisions shape my future, and though mistakes are inevitable, they define who I become.\"\n",
        "\n",
        "# Calculate the maximum activations for each prompt using the feature index\n",
        "max_activation_a = get_max_activation(model, tokenizer, sae, prompt_a, feature_idx=feature_idx)\n",
        "max_activation_b = get_max_activation(model, tokenizer, sae, prompt_b, feature_idx=feature_idx)\n",
        "\n",
        "# Print the comparison\n",
        "print(f\"max_activation for prompt_a: {max_activation_a}\")\n",
        "print(f\"max_activation for prompt_b: {max_activation_b}\")\n",
        "###########################################################"
      ],
      "metadata": {
        "id": "iLuW-Hk55qJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q7.4~7.6: Activation distribution for specific layer"
      ],
      "metadata": {
        "id": "gFg9hOKzGW8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_token_activations(model, tokenizer, sae, prompt, feature_idx=10004, layer_idx=0):\n",
        "    \"\"\"\n",
        "    Plots activations for each token in a specific layer.\n",
        "\n",
        "    Args:\n",
        "        model: The transformer model.\n",
        "        tokenizer: Tokenizer for encoding input text.\n",
        "        sae: Sparse Autoencoder model.\n",
        "        prompt: Input text string.\n",
        "        feature_idx: Index of the feature to analyze.\n",
        "        layer_idx: Layer to analyze (None uses sae.cfg.hook_layer).\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    sae.to(device)\n",
        "\n",
        "    # Tokenize input and get model output\n",
        "    tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    token_ids = tokens[\"input_ids\"].to(device)\n",
        "    token_list = tokenizer.convert_ids_to_tokens(token_ids.squeeze().tolist())\n",
        "\n",
        "    outputs = model(token_ids, output_hidden_states=True)\n",
        "\n",
        "    # Choose layer\n",
        "    layer_idx = layer_idx if layer_idx is not None else sae.cfg.hook_layer\n",
        "    hidden_states = outputs.hidden_states[layer_idx]\n",
        "\n",
        "    # Pass through SAE\n",
        "    sae_in = hidden_states\n",
        "    feature_acts = sae.encode(sae_in).squeeze()  # (batch_size, seq_len, num_features)\n",
        "    print(f\"feature_acts shape: {feature_acts.shape}\")\n",
        "\n",
        "    # Extract activations for the chosen feature\n",
        "    activations = feature_acts[:, feature_idx].squeeze().cpu().detach().numpy()\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(range(len(token_list)), activations, color='blue', alpha=0.7)\n",
        "    plt.xticks(range(len(token_list)), token_list, rotation=45)\n",
        "    plt.xlabel(\"Tokens\")\n",
        "    plt.ylabel(f\"Activation Value (Feature {feature_idx})\")\n",
        "    plt.title(f\"Token-wise Activations for Layer {layer_idx}\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "######################## (Q7.4 ~ 7.6) ########################\n",
        "# Simply observe the figure\n",
        "layer_idx = 24\n",
        "prompt = \"Time travel will become a reality as technology continues to advance.\"\n",
        "plot_token_activations(model, tokenizer, sae, prompt_a, feature_idx, layer_idx)\n",
        "###################################################################"
      ],
      "metadata": {
        "id": "Ms4lv2S552IS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q7.7~7.9: Activation distribution for specific token"
      ],
      "metadata": {
        "id": "EwLP2sXZGcrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_layer_activations(model, tokenizer, sae, prompt, token_idx=0, feature_idx=10004):\n",
        "    \"\"\"\n",
        "    Plots activations of a specific token across all layers.\n",
        "\n",
        "    Args:\n",
        "        model: The transformer model.\n",
        "        tokenizer: Tokenizer for encoding input text.\n",
        "        sae: Sparse Autoencoder model.\n",
        "        prompt: Input text string.\n",
        "        token_idx: Index of the token to analyze.\n",
        "        feature_idx: Index of the feature to analyze.\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    sae.to(device)\n",
        "\n",
        "    # Tokenize input and get model output\n",
        "    tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    token_ids = tokens[\"input_ids\"].to(device)\n",
        "    token_list = tokenizer.convert_ids_to_tokens(token_ids.squeeze().tolist())\n",
        "\n",
        "    outputs = model(token_ids, output_hidden_states=True)\n",
        "\n",
        "    # Collect activations across all layers\n",
        "    num_layers = len(outputs.hidden_states)\n",
        "    activations = []\n",
        "\n",
        "    for layer_idx in range(num_layers):\n",
        "        hidden_states = outputs.hidden_states[layer_idx]\n",
        "        sae_in = hidden_states\n",
        "        feature_acts = sae.encode(sae_in).squeeze()  # (batch_size, seq_len, num_features)\n",
        "        # print(f\"feature_acts shape: {feature_acts.shape}\")\n",
        "        activations.append(feature_acts[token_idx, feature_idx].item())\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(range(num_layers), activations, marker=\"o\", linestyle=\"-\", color=\"blue\")\n",
        "    plt.xlabel(\"Layer\")\n",
        "    plt.ylabel(f\"Activation Value (Feature {feature_idx})\")\n",
        "    plt.title(f\"Activation Across Layers for Token '{token_list[token_idx]}'\")\n",
        "    plt.xticks(range(num_layers))\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "######################## (Q7.7 ~ 7.9) ########################\n",
        "# Alter the token index to observe the figure\n",
        "token_idx = 1\n",
        "prompt = \"Time travel will become a reality as technology continues to advance.\"\n",
        "plot_layer_activations(model, tokenizer, sae, prompt, token_idx)\n",
        "###################################################################"
      ],
      "metadata": {
        "id": "WjssMqTi55km"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30840,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}