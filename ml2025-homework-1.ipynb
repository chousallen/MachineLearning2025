{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ML2025 Homework 1 - Retrieval Augmented Generation with Agents","metadata":{"id":"1TFwaJir_Olj"}},{"cell_type":"markdown","source":"## Environment Setup","metadata":{"id":"6tQHdH2k_Olk"}},{"cell_type":"markdown","source":"In this section, we install the necessary python packages and download model weights of the quantized version of LLaMA 3.1 8B. Also, download the dataset. Note that the model weight is around 8GB.","metadata":{"id":"mGx000oZ_Oll"}},{"cell_type":"code","source":"!python3 -m pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n!python3 -m pip install googlesearch-python bs4 charset-normalizer requests-html lxml_html_clean\n\nfrom pathlib import Path\nif not Path('./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf').exists():\n    !wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\nif not Path('./public.txt').exists():\n    !wget https://www.csie.ntu.edu.tw/~ulin/public.txt\nif not Path('./private.txt').exists():\n    !wget https://www.csie.ntu.edu.tw/~ulin/private.txt","metadata":{"execution":{"iopub.status.busy":"2025-03-06T09:37:15.891146Z","iopub.execute_input":"2025-03-06T09:37:15.891478Z","iopub.status.idle":"2025-03-06T09:37:22.410888Z","shell.execute_reply.started":"2025-03-06T09:37:15.891448Z","shell.execute_reply":"2025-03-06T09:37:22.409981Z"},"id":"5JywoPOO_Oll","trusted":true},"outputs":[{"name":"stdout","text":"Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu122\nRequirement already satisfied: llama-cpp-python==0.3.4 in /usr/local/lib/python3.10/dist-packages (0.3.4)\nRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.3.4) (4.12.2)\nRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.3.4) (1.26.4)\nRequirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.3.4) (5.6.3)\nRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.3.4) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.3.4) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python==0.3.4) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python==0.3.4) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.20.0->llama-cpp-python==0.3.4) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.20.0->llama-cpp-python==0.3.4) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.20.0->llama-cpp-python==0.3.4) (2024.2.0)\nRequirement already satisfied: googlesearch-python in /usr/local/lib/python3.10/dist-packages (1.3.0)\nRequirement already satisfied: bs4 in /usr/local/lib/python3.10/dist-packages (0.0.2)\nRequirement already satisfied: charset-normalizer in /usr/local/lib/python3.10/dist-packages (3.4.1)\nRequirement already satisfied: requests-html in /usr/local/lib/python3.10/dist-packages (0.10.0)\nRequirement already satisfied: lxml_html_clean in /usr/local/lib/python3.10/dist-packages (0.4.1)\nRequirement already satisfied: beautifulsoup4>=4.9 in /usr/local/lib/python3.10/dist-packages (from googlesearch-python) (4.12.3)\nRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from googlesearch-python) (2.32.3)\nRequirement already satisfied: pyquery in /usr/local/lib/python3.10/dist-packages (from requests-html) (2.0.1)\nRequirement already satisfied: fake-useragent in /usr/local/lib/python3.10/dist-packages (from requests-html) (2.0.3)\nRequirement already satisfied: parse in /usr/local/lib/python3.10/dist-packages (from requests-html) (1.20.2)\nRequirement already satisfied: w3lib in /usr/local/lib/python3.10/dist-packages (from requests-html) (2.3.1)\nRequirement already satisfied: pyppeteer>=0.0.14 in /usr/local/lib/python3.10/dist-packages (from requests-html) (2.0.0)\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from lxml_html_clean) (5.3.0)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.6)\nRequirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (1.4.4)\nRequirement already satisfied: certifi>=2023 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (2025.1.31)\nRequirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (8.5.0)\nRequirement already satisfied: pyee<12.0.0,>=11.0.0 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (11.1.1)\nRequirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (4.67.1)\nRequirement already satisfied: urllib3<2.0.0,>=1.25.8 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (1.26.20)\nRequirement already satisfied: websockets<11.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (10.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->googlesearch-python) (3.10)\nRequirement already satisfied: cssselect>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyquery->requests-html) (1.2.0)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.21.0)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyee<12.0.0,>=11.0.0->pyppeteer>=0.0.14->requests-html) (4.12.2)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import torch\nif not torch.cuda.is_available():\n    raise Exception('You are not using the GPU runtime. Change it first or you will suffer from the super slow inference speed!')\nelse:\n    print('You are good to go!')","metadata":{"execution":{"iopub.status.busy":"2025-03-06T09:37:22.412323Z","iopub.execute_input":"2025-03-06T09:37:22.412565Z","iopub.status.idle":"2025-03-06T09:37:22.417218Z","shell.execute_reply.started":"2025-03-06T09:37:22.412543Z","shell.execute_reply":"2025-03-06T09:37:22.416343Z"},"id":"kX6SizAt_Olm","trusted":true},"outputs":[{"name":"stdout","text":"You are good to go!\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Prepare the LLM and LLM utility function","metadata":{"id":"l3iyc1qC_Olm"}},{"cell_type":"markdown","source":"By default, we will use the quantized version of LLaMA 3.1 8B. you can get full marks on this homework by using the provided LLM and LLM utility function. You can also try out different LLM models.","metadata":{"id":"T59vxAo2_Olm"}},{"cell_type":"markdown","source":"In the following code block, we will load the downloaded LLM model weights onto the GPU first.\nThen, we implemented the generate_response() function so that you can get the generated response from the LLM model more easily.","metadata":{"id":"vtepTeT3_Olm"}},{"cell_type":"markdown","source":"You can ignore \"llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\" warning.","metadata":{"id":"eVil2Vhe_Olm"}},{"cell_type":"code","source":"from llama_cpp import Llama\n\n# Load the model onto GPU\nllama3 = Llama(\n    \"./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\",\n    verbose=False,\n    n_gpu_layers=-1,\n    n_ctx=16384,    # This argument is how many tokens the model can take. The longer the better, but it will consume more memory. 16384 is a proper value for a GPU with 16GB VRAM.\n)\n\ndef generate_response(_model: Llama, _messages: str) -> str:\n    '''\n    This function will inference the model with given messages.\n    '''\n    _output = _model.create_chat_completion(\n        _messages,\n        stop=[\"<|eot_id|>\", \"<|end_of_text|>\"],\n        max_tokens=512,    # This argument is how many tokens the model can generate.\n        temperature=0,      # This argument is the randomness of the model. 0 means no randomness. You will get the same result with the same input every time. You can try to set it to different values.\n        repeat_penalty=2.0,\n    )[\"choices\"][0][\"message\"][\"content\"]\n    return _output","metadata":{"execution":{"iopub.status.busy":"2025-03-06T09:37:22.418672Z","iopub.execute_input":"2025-03-06T09:37:22.418921Z","iopub.status.idle":"2025-03-06T09:37:23.855915Z","shell.execute_reply.started":"2025-03-06T09:37:22.418891Z","shell.execute_reply":"2025-03-06T09:37:23.854501Z"},"id":"ScyW45N__Olm","trusted":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-08e43956545c>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the model onto GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m llama3 = Llama(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, n_gpu_layers, split_mode, main_gpu, tensor_split, rpc_servers, vocab_only, use_mmap, use_mlock, kv_overrides, seed, n_ctx, n_batch, n_ubatch, n_threads, n_threads_batch, rope_scaling_type, pooling_type, rope_freq_base, rope_freq_scale, yarn_ext_factor, yarn_attn_factor, yarn_beta_fast, yarn_beta_slow, yarn_orig_ctx, logits_all, embedding, offload_kqv, flash_attn, last_n_tokens_size, lora_base, lora_scale, lora_path, numa, chat_format, chat_handler, draft_model, tokenizer, type_k, type_v, spm_infill, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m         self._model = self._stack.enter_context(\n\u001b[1;32m    368\u001b[0m             contextlib.closing(\n\u001b[0;32m--> 369\u001b[0;31m                 internals.LlamaModel(\n\u001b[0m\u001b[1;32m    370\u001b[0m                     \u001b[0mpath_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m                     \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/_internals.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_model, params, verbose)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed to load model from file: {path_model}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Failed to load model from file: ./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf"],"ename":"ValueError","evalue":"Failed to load model from file: ./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf","output_type":"error"}],"execution_count":13},{"cell_type":"markdown","source":"## Search Tool","metadata":{"id":"tnHLwq-4_Olm"}},{"cell_type":"markdown","source":"The TA has implemented a search tool for you to search certain keywords using Google Search. You can use this tool to search for the relevant **web pages** for the given question. The search tool can be integrated in the following sections.","metadata":{"id":"SYM-2ZsE_Olm"}},{"cell_type":"code","source":"from typing import List\nfrom googlesearch import search as _search\nfrom bs4 import BeautifulSoup\nfrom charset_normalizer import detect\nimport asyncio\nfrom requests_html import AsyncHTMLSession\nimport urllib3\nurllib3.disable_warnings()\n\nasync def worker(s:AsyncHTMLSession, url:str):\n    try:\n        header_response = await asyncio.wait_for(s.head(url, verify=False), timeout=10)\n        if 'text/html' not in header_response.headers.get('Content-Type', ''):\n            return None\n        r = await asyncio.wait_for(s.get(url, verify=False), timeout=10)\n        return r.text\n    except:\n        return None\n\nasync def get_htmls(urls):\n    session = AsyncHTMLSession()\n    tasks = (worker(session, url) for url in urls)\n    return await asyncio.gather(*tasks)\n\nasync def search(keyword: str, n_results: int=1) -> List[str]:\n    '''\n    This function will search the keyword and return the text content in the first n_results web pages.\n    Warning: You may suffer from HTTP 429 errors if you search too many times in a period of time. This is unavoidable and you should take your own risk if you want to try search more results at once.\n    The rate limit is not explicitly announced by Google, hence there's not much we can do except for changing the IP or wait until Google unban you (we don't know how long the penalty will last either).\n    '''\n    keyword = keyword[:100]\n    # First, search the keyword and get the results. Also, get 2 times more results in case some of them are invalid.\n    results = list(_search(keyword, n_results * 2, lang=\"zh\", unique=True))\n    # Then, get the HTML from the results. Also, the helper function will filter out the non-HTML urls.\n    results = await get_htmls(results)\n    # Filter out the None values.\n    results = [x for x in results if x is not None]\n    # Parse the HTML.\n    results = [BeautifulSoup(x, 'html.parser') for x in results]\n    # Get the text from the HTML and remove the spaces. Also, filter out the non-utf-8 encoding.\n    results = [''.join(x.get_text().split()) for x in results if detect(x.encode()).get('encoding') == 'utf-8']\n    # Return the first n results.\n    return results[:n_results]","metadata":{"execution":{"iopub.status.busy":"2025-03-06T09:41:23.767149Z","iopub.execute_input":"2025-03-06T09:41:23.767479Z","iopub.status.idle":"2025-03-06T09:41:23.774905Z","shell.execute_reply.started":"2025-03-06T09:41:23.767453Z","shell.execute_reply":"2025-03-06T09:41:23.774093Z"},"id":"bEIRmZl7_Oln","trusted":true},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"## Test the LLM inference pipeline","metadata":{"id":"rC3zQjjj_Oln"}},{"cell_type":"code","source":"# You can try out different questions here.\ntest_question='請問誰是 Taylor Swift？'\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\"},    # System prompt\n    {\"role\": \"user\", \"content\": test_question}, # User prompt\n]\n\nprint(generate_response(llama3, messages))","metadata":{"execution":{"iopub.status.busy":"2025-03-06T09:40:13.911638Z","iopub.execute_input":"2025-03-06T09:40:13.911913Z","iopub.status.idle":"2025-03-06T09:40:22.935657Z","shell.execute_reply.started":"2025-03-06T09:40:13.911893Z","shell.execute_reply":"2025-03-06T09:40:22.934857Z"},"id":"8dmGCARd_Oln","trusted":true},"outputs":[{"name":"stdout","text":"泰勒絲（Taylor Swift）是一位美國歌手、詞曲作家和製作人。她出生於1989年，來自田納西州。她的音樂風格從鄉村搖滾開始逐漸轉變為流行電音。\n\n她早期的作品如《泰勒絲第一輯》、《愛情故事第二章：睡美人的秘密》，獲得了廣泛認可和獎項，包括多個告示牌音樂大奖。後來，她推出了更具商業成功性的專辑，如 《1989》（2014）、_reputation（《名聲_(泰勒絲专輯)》） （ 20 ） 和 _Lover(2020)，並且在全球取得了巨大的影響力。\n\n她以她的歌曲如 \"Shake It Off\"、\"_Blank Space_\"和 \"_Bad Blood_\",以及與其他藝人合作的作品，如 《Look What You Made Me Do》（2017）而聞名。泰勒絲還是知識產權運動的一部分，對於音樂創作者在數位時代獲得公平報酬有所關注。\n\n她被譽為當代最成功和影響力最大的人物之一，並且她的歌曲經常成為流行文化的話題。\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## Agents","metadata":{"id":"C0-ojJuE_Oln"}},{"cell_type":"markdown","source":"The TA has implemented the Agent class for you. You can use this class to create agents that can interact with the LLM model. The Agent class has the following attributes and methods:\n- Attributes:\n    - role_description: The role of the agent. For example, if you want this agent to be a history expert, you can set the role_description to \"You are a history expert. You will only answer questions based on what really happened in the past. Do not generate any answer if you don't have reliable sources.\".\n    - task_description: The task of the agent. For example, if you want this agent to answer questions only in yes/no, you can set the task_description to \"Please answer the following question in yes/no. Explanations are not needed.\"\n    - llm: Just an indicator of the LLM model used by the agent.\n- Method:\n    - inference: This method takes a message as input and returns the generated response from the LLM model. The message will first be formatted into proper input for the LLM model. (This is where you can set some global instructions like \"Please speak in a polite manner\" or \"Please provide a detailed explanation\".) The generated response will be returned as the output.","metadata":{"id":"HGsIPud3_Oln"}},{"cell_type":"code","source":"class LLMAgent():\n    def __init__(self, role_description: str, task_description: str, llm:str=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\"):\n        self.role_description = role_description   # Role means who this agent should act like. e.g. the history expert, the manager......\n        self.task_description = task_description    # Task description instructs what task should this agent solve.\n        self.llm = llm  # LLM indicates which LLM backend this agent is using.\n    def inference(self, message:str) -> str:\n        if self.llm == 'bartowski/Meta-Llama-3.1-8B-Instruct-GGUF': # If using the default one.\n            # TODO: Design the system prompt and user prompt here.\n            # Format the messsages first.\n            messages = [\n                {\"role\": \"system\", \"content\": f\"{self.role_description}\"},  # Hint: you may want the agents to speak Traditional Chinese only.\n                {\"role\": \"user\", \"content\": f\"{self.task_description}\\n{message}\"}, # Hint: you may want the agents to clearly distinguish the task descriptions and the user messages. A proper seperation text rather than a simple line break is recommended.\n            ]\n            return generate_response(llama3, messages)\n        else:\n            # TODO: If you want to use LLMs other than the given one, please implement the inference part on your own.\n            return \"\"","metadata":{"execution":{"iopub.status.busy":"2025-03-06T09:40:27.797762Z","iopub.execute_input":"2025-03-06T09:40:27.798096Z","iopub.status.idle":"2025-03-06T09:40:27.803494Z","shell.execute_reply.started":"2025-03-06T09:40:27.798070Z","shell.execute_reply":"2025-03-06T09:40:27.802366Z"},"id":"zjG-UwDX_Oln","trusted":true},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"TODO 1: Design the role description and task description for each agent.","metadata":{"id":"0-ueJrgP_Oln"}},{"cell_type":"code","source":"# TODO: Design the role and task description for each agent.\n\n# This agent may help you filter out the irrelevant parts in question descriptions.\nquestion_extraction_agent = LLMAgent(\n    role_description=\"你是 LLaMA-3.1-8B，是從問題描述中提取唯一一個關鍵問題的 AI。\",\n    task_description=\"請從以下描述中提取關鍵問題：\",\n)\n\n# This agent may help you extract the keywords in a question so that the search tool can find more accurate results.\nkeyword_extraction_agent = LLMAgent(\n    role_description=\"你是 LLaMA-3.1-8B，是從問題描述中找出關鍵詞的 AI。\",\n    task_description=\"請提取以下描述中的關鍵詞：\",\n)\n\n# This agent is the core component that answers the question.\nqa_agent = LLMAgent(\n    role_description=\"你是 LLaMA-3.1-8B，是根據提供的相關資料回答問題的 AI。\",\n    task_description=\"請根據提供的相關資料回答以下問題：\",\n)","metadata":{"execution":{"iopub.status.busy":"2025-03-06T09:40:30.448830Z","iopub.execute_input":"2025-03-06T09:40:30.449115Z","iopub.status.idle":"2025-03-06T09:40:30.453299Z","shell.execute_reply.started":"2025-03-06T09:40:30.449093Z","shell.execute_reply":"2025-03-06T09:40:30.452503Z"},"id":"DzPzmNnj_Oln","trusted":true},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## RAG pipeline","metadata":{"id":"A9eoywr7_Oln"}},{"cell_type":"markdown","source":"TODO 2: Implement the RAG pipeline.","metadata":{"id":"8HDOjNYJ_Oln"}},{"cell_type":"markdown","source":"Please refer to the homework description slides for hints.\n\nAlso, there might be more heuristics (e.g. classifying the questions based on their lengths, determining if the question need a search or not, reconfirm the answer before returning it to the user......) that are not shown in the flow charts. You can use your creativity to come up with a better solution!","metadata":{"id":"MRGNa-1i_Oln"}},{"cell_type":"markdown","source":"- Naive approach (simple baseline)\n\n    ![](https://www.csie.ntu.edu.tw/~ulin/naive.png)","metadata":{"id":"cMaIsKAZ_Olo"}},{"cell_type":"markdown","source":"- Naive RAG approach (medium baseline)\n\n    ![](https://www.csie.ntu.edu.tw/~ulin/naive_rag.png)","metadata":{"id":"mppO-oOO_Olo"}},{"cell_type":"markdown","source":"- RAG with agents (strong baseline)\n\n    ![](https://www.csie.ntu.edu.tw/~ulin/rag_agent.png)","metadata":{"id":"HYxbciLO_Olo"}},{"cell_type":"code","source":"async def pipeline(question: str) -> str:\n    print(f\"[Original Question]: {question}\\n\")\n    \n    # Extract keywords from the question using the keyword extraction agent\n    keywords = keyword_extraction_agent.inference(question)\n    print(f\"[Keywords]: {keywords}\\n\")\n    \n    # Use the search tool to get information based on the extracted keywords\n    search_results = await search(keywords)\n    #print(f\"[Search Results]: {search_results}\")\n    \n    # Combine the search results into a single string\n    combined_search_results = \" \".join(search_results)\n    #print(f\"[Combined Search Results]: {combined_search_results}\")\n    \n    # Extract the refined question using the question extraction agent\n    refined_question = question_extraction_agent.inference(question)\n    print(f\"[Refined Question]: {refined_question}\\n\")\n    \n    # Combine the refined question with the search results\n    combined_input = f\"{refined_question}\\n{combined_search_results}\"\n    \n    # Get the final answer from the QA agent\n    final_answer = qa_agent.inference(combined_input)\n    \n    return final_answer","metadata":{"execution":{"iopub.status.busy":"2025-03-06T09:40:37.056678Z","iopub.execute_input":"2025-03-06T09:40:37.056951Z","iopub.status.idle":"2025-03-06T09:40:37.061689Z","shell.execute_reply.started":"2025-03-06T09:40:37.056931Z","shell.execute_reply":"2025-03-06T09:40:37.060838Z"},"id":"ztJkA7R7_Olo","trusted":true},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## Answer the questions using your pipeline!","metadata":{"id":"P_kI_9EGB0S9"}},{"cell_type":"markdown","source":"Since Colab has usage limit, you might encounter the disconnections. The following code will save your answer for each question. If you have mounted your Google Drive as instructed, you can just rerun the whole notebook to continue your process.","metadata":{"id":"PN17sSZ8DUg7"}},{"cell_type":"code","source":"from pathlib import Path\n\n# Fill in your student ID first.\nSTUDENT_ID = \"B12901148\"\n\nSTUDENT_ID = STUDENT_ID.lower()\nwith open('./public.txt', 'r') as input_f:\n    questions = input_f.readlines()\n    questions = [l.strip().split(',')[0] for l in questions]\n    for id, question in enumerate(questions, 1):\n        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n            continue\n        answer = await pipeline(question)\n        answer = answer.replace('\\n',' ')\n        print(id, answer)\n        with open(f'./{STUDENT_ID}_{id}.txt', 'w') as output_f:\n            print(answer, file=output_f)\n\nwith open('./private.txt', 'r') as input_f:\n    questions = input_f.readlines()\n    for id, question in enumerate(questions, 31):\n        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n            continue\n        answer = await pipeline(question)\n        answer = answer.replace('\\n',' ')\n        print(id, answer)\n        with open(f'./{STUDENT_ID}_{id}.txt', 'a') as output_f:\n            print(answer, file=output_f)","metadata":{"execution":{"iopub.status.busy":"2025-03-06T09:41:31.019603Z","iopub.execute_input":"2025-03-06T09:41:31.019878Z","iopub.status.idle":"2025-03-06T09:42:00.563102Z","shell.execute_reply.started":"2025-03-06T09:41:31.019858Z","shell.execute_reply":"2025-03-06T09:42:00.561838Z"},"id":"plUDRTi_B39S","trusted":true},"outputs":[{"name":"stdout","text":"[Original Question]: Rugby Union 中觸地 try 可得幾分？\n\n[Keywords]: 關鍵詞：Rugby Union、觸地 try\n\n[Refined Question]: 關鍵問題是：\n觸地 try 在 Rugby Union 中可得幾分？\n\n5 根據提供的資料，觸地 try 在 Rugby Union 中可得 5 分。\n[Original Question]: 卑南族是位在臺東平原的一個原住民族，以驍勇善戰、擅長巫術聞名，曾經統治整個臺東平原。相傳卑南族的祖先發源自 ruvuwa'an，該地位於現今的哪個行政區劃？\n\n[Keywords]: 關鍵詞：\n\n1. 卑南族\n2.. 臺東平原 \n3.ruvuwa'an\n\n[Refined Question]: 關鍵問題是：\n該地 ruvuwa'an 位置在哪個行政區劃？\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-fb7599cec666>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"./{STUDENT_ID}_{id}.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-57abf9b5da02>\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Get the final answer from the QA agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mfinal_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqa_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfinal_answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-5e332f867d91>\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"{self.task_description}\\n{message}\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Hint: you may want the agents to clearly distinguish the task descriptions and the user messages. A proper seperation text rather than a simple line break is recommended.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             ]\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllama3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# TODO: If you want to use LLMs other than the given one, please implement the inference part on your own.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-08e43956545c>\u001b[0m in \u001b[0;36mgenerate_response\u001b[0;34m(_model, _messages)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mwill\u001b[0m \u001b[0minference\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     '''\n\u001b[0;32m---> 15\u001b[0;31m     _output = _model.create_chat_completion(\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0m_messages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<|eot_id|>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"<|end_of_text|>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36mcreate_chat_completion\u001b[0;34m(self, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, logprobs, top_logprobs)\u001b[0m\n\u001b[1;32m   1996\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0mllama_chat_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_chat_completion_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m         )\n\u001b[0;32m-> 1998\u001b[0;31m         return handler(\n\u001b[0m\u001b[1;32m   1999\u001b[0m             \u001b[0mllama\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama_chat_format.py\u001b[0m in \u001b[0;36mchat_completion_handler\u001b[0;34m(llama, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, logprobs, top_logprobs, **kwargs)\u001b[0m\n\u001b[1;32m    660\u001b[0m                 )\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m         completion_or_chunks = llama.create_completion(\n\u001b[0m\u001b[1;32m    663\u001b[0m             \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36mcreate_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1830\u001b[0m             \u001b[0mchunks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCreateCompletionStreamResponse\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompletion_or_chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1831\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1832\u001b[0;31m         \u001b[0mcompletion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCompletion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompletion_or_chunks\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1833\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompletion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36m_create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_tokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1268\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1269\u001b[0m                 \u001b[0;34mf\"Requested tokens ({len(prompt_tokens)}) exceed context window of {llama_cpp.llama_n_ctx(self.ctx)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m             )\n","\u001b[0;31mValueError\u001b[0m: Requested tokens (18066) exceed context window of 16384"],"ename":"ValueError","evalue":"Requested tokens (18066) exceed context window of 16384","output_type":"error"}],"execution_count":25},{"cell_type":"code","source":"# Combine the results into one file.\nwith open(f'./{STUDENT_ID}.txt', 'w') as output_f:\n    for id in range(1,91):\n        with open(f'./{STUDENT_ID}_{id}.txt', 'r') as input_f:\n            answer = input_f.readline().strip()\n            print(answer, file=output_f)","metadata":{"execution":{"iopub.status.busy":"2025-03-06T09:37:23.864177Z","iopub.status.idle":"2025-03-06T09:37:23.864480Z","shell.execute_reply":"2025-03-06T09:37:23.864370Z"},"id":"GmLO9PlmEBPn","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}